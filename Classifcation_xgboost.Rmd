---
title: "Classification attempt 1"
author: "Liam Martin-McClay"
date: "2024-07-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidymodels)

train <- read_csv("train.csv")
test <- read_csv("test.csv")
train <- train %>%
  mutate_if(is.character, factor)
train <- train[,-4]
test <- test %>%
  mutate_if(is.character, factor)
```

```{r}
#visualization that I want to come back to

```

```{r}
#no data preprocessing. The exports say no preprocessing, but takes times to tune
#I need to read up on exactly what each of these hyperparamaters do
#min_n, loss, and tree_depth has to deal with model complexity
#sample_size, and mtry are related to the randomness involved
xgb_spec <- boost_tree(trees = 1000,
           tree_depth = tune(), min_n = tune(), 
           loss_reduction = tune(), sample_size = tune(), 
           mtry = tune(),learn_rate = tune()
) %>% 
  set_engine("xgboost") %>%
  set_mode("classification")
```

```{r}
#grid_regular method would take too long with this many tuned parameters
#grid_latin_hypercube evenly spaces out different models in the n(six in this case) dimensional space, attempting to cover the space equally

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  learn_rate(),
  finalize(mtry(), train),
  size = 10
)
```

```{r}
xgb_wf <- workflow() %>%
  add_formula(winner~.) %>%
  add_model(xgb_spec)
xgb_wf
```
Cross validation
```{r}
set.seed(101)
vb_folds <- vfold_cv(train, strata = winner)
```

```{r}
doParallel::registerDoParallel()

set.seed(202)

xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE, verbose = TRUE)
)
```


```{r}
xgb_res %>%
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>%
  select(mean,mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               names_to = "parameter",
               values_to = "value") %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
facet_wrap(~parameter, scales = "free_x")
```


```{r}
#just helpful to see
show_best(xgb_res, metric = "roc_auc")
best_auc <- select_best(xgb_res, metric = "roc_auc")
final_xgb <- finalize_workflow(xgb_wf, best_auc)
final_xgb
```

```{r}
#useless graph, too many predictors
library(vip)
final_xgb %>%
  fit(data = train) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

```{r}
final_res <- final_xgb %>%
  fit(data = train)
predictions <- final_res %>%
  predict(new_data = test)
```

```{r}
pred_table_2 <- bind_cols(test %>% select(id), predictions)

write_csv(pred_table_2, "xgb_predictions_tuned.csv")
```

